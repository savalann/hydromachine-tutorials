{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/envs/devcon_2025/lib/python3.12/site-packages/pyproj/__init__.py:89: UserWarning: pyproj unable to set database path.\n",
      "  _pyproj_global_context_initialize()\n"
     ]
    }
   ],
   "source": [
    "# my packages\n",
    "from evaluation_table import EvalTable\n",
    "from figure_generator import EvalPlot\n",
    "from model import CustomBiLSTM\n",
    "from tuning_tools import tuning_game, tune_model \n",
    "from data_preprocess import data_prepare, data_split\n",
    "from final_eval import general_viz, regime_eval, signature_eval, eval_drought\n",
    "\n",
    "# basic packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import math\n",
    "import joblib\n",
    "\n",
    "# system packages\n",
    "from datetime import datetime, date, timedelta\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import platform\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# hydrological packages\n",
    "import hydroeval as he\n",
    "from hydrotools.nwm_client import utils # I had to pip install this\n",
    "\n",
    "# data analysis packages\n",
    "from scipy import optimize\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# deep learning packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Identify the path\n",
    "home = os.getcwd()\n",
    "parent_path = os.path.dirname(home)\n",
    "input_path = f'{parent_path}/02.input/'\n",
    "output_path = f'{parent_path}/03.output/'\n",
    "main_path = home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jovyan/mydrive/devcon_2025/hydromachine-tutorials/neural_nets/lstm/03.output/best_hyperparameters_lstm.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43mbest_hyperparameters_lstm.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the train and test dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m data_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mtrain_dataset.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/envs/devcon_2025/lib/python3.12/site-packages/joblib/numpy_pickle.py:650\u001b[0m, in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    648\u001b[0m         obj \u001b[38;5;241m=\u001b[39m _unpickle(fobj)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    651\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _read_fileobject(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m fobj:\n\u001b[1;32m    652\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    653\u001b[0m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[1;32m    654\u001b[0m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[1;32m    655\u001b[0m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jovyan/mydrive/devcon_2025/hydromachine-tutorials/neural_nets/lstm/03.output/best_hyperparameters_lstm.pkl'"
     ]
    }
   ],
   "source": [
    "params = joblib.load(f'{output_path}best_hyperparameters_lstm.pkl')\n",
    "\n",
    "# Load the train and test dataset\n",
    "data_train = pd.read_pickle(f\"{output_path}train_dataset.pkl\")\n",
    "data_test = pd.read_pickle(f\"{output_path}test_dataset.pkl\")\n",
    "dataset = pd.read_pickle(f\"{output_path}dataset.pkl\")\n",
    "\n",
    "station_list = list(data_test.station_id.unique())\n",
    "\n",
    "length_lookback = 10\n",
    "x_train_scaled, y_train_scaled, x_test_scaled, y_test_scaled, scaler_x, scaler_y, y_train, x_test, y_test = data_prepare(data_train, data_test, length_lookback=length_lookback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Training\n",
    "#### 5.1 Training\n",
    "- If we want to use GPUs to run the model before we feed the data to the training fucntion we have to transfer the data and the model to the GPU.\n",
    "- Then we will use the TensorDataset function as wrapper for our feature and target to combine them together.   \n",
    "- Next, we should use the DataLoader function of the Pytorch library. DataLoader automatically creates minibatches of the dataset for the training process and speeds up the data loading process by parallelizing the loading of data from disc to the GPU/CPU. \n",
    "- We will use Adam optimizer for calculating the weights and biases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = params['batch_size']\n",
    "learning_rate = params['learning_rate']\n",
    "hidden_size = params['hidden_size']\n",
    "num_layers = params['num_layers']\n",
    "input_size = x_train_scaled[list(x_train_scaled.keys())[0]].shape[2]\n",
    "path_model = f'{output_path}best_model.pth'\n",
    "\n",
    "# Move the model and data to GPU. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader= {}\n",
    "\n",
    "for station_id in station_list:\n",
    "\n",
    "    x_train_tensor = torch.Tensor(x_train_scaled[station_id].astype(float))\n",
    "    y_train_tensor = torch.Tensor(y_train_scaled[station_id].astype(float))\n",
    "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "    train_loader[station_id] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create the Model\n",
    "bilstm_model = CustomBiLSTM(input_size, hidden_size, num_layers, 1, device, embedding=False, station_list=station_list)\n",
    "\n",
    "# Create the Optimizer\n",
    "bilstm_optimizer = optim.Adam(bilstm_model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "\n",
    "# Run the training function\n",
    "model_parameters = bilstm_model.train_model(train_loader, epochs, bilstm_optimizer, early_stopping_patience=0, val_loader=None)\n",
    "\n",
    "# Save the Model Parameters\n",
    "bilstm_model.save_model(path_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Training Tricks\n",
    "* **`early_stop`** Early stopping is a regularization technique that stops training when the modelâ€™s performance on a validation set stops improving, helping to prevent overfitting.\n",
    "\n",
    "* **`weight_decay`** Weight decay is a form of L2 regularization that penalizes large weights by adding a term to the loss function, encouraging the model to keep its parameters small and generalize better.\n",
    "\n",
    "* **`embedding`** Embedding is a technique used to represent categorical variables or high-dimensional data (like words or spatial identifiers) as dense, lower-dimensional vectors. In neural networks, embeddings help capture relationships or similarities between inputs in a form that models can learn from efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation \n",
    "#### 6.1. Model Evaluation Metrics\n",
    "- We fist have to transform the results to their original scale. \n",
    "- To evaluate our model we will use KGE, RMSE, and PBias metrics. \n",
    "- We will also compare the cumulative streamflow in each year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty DataFrames to store evaluation results if not already defined.\n",
    "EvalDF_all_rf = pd.DataFrame()\n",
    "SupplyEvalDF_all_rf = pd.DataFrame()\n",
    "df_eval_rf = pd.DataFrame()\n",
    "df_result_data= {}\n",
    "\n",
    "\n",
    "# Iterate over each station name in the list of station IDs.\n",
    "for station_name in station_list:\n",
    "    x_test_tensor = torch.Tensor(x_test_scaled[station_name].astype(float))\n",
    "    y_test_tensor = torch.Tensor(y_test_scaled[station_name].astype(float))\n",
    "    test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=test_dataset.tensors[0].shape[0], shuffle=False)\n",
    "    yhat_test_scaled, val_loss = bilstm_model.evaluate_model(test_loader)\n",
    "    \n",
    "    # Inverse transform the scaled predictions to their original scale.\n",
    "    yhat_test = scaler_y.inverse_transform(yhat_test_scaled.reshape(-1, 1))\n",
    "    \n",
    "    # Assuming EvalTable is a predefined function that compares predictions to actuals and returns evaluation DataFrames.\n",
    "    EvalDF_all_rf_temp, SupplyEvalDF_all_rf_temp, df_eval_rf_temp = EvalTable(yhat_test.reshape(-1), data_test[data_test.station_id == station_name][length_lookback:], 'lstm')\n",
    "\n",
    "    df_result_data[station_name] = data_test[data_test.station_id == station_name][length_lookback:].copy()\n",
    "\n",
    "    df_result_data[station_name]['lstm_flow'] = yhat_test\n",
    "\n",
    "    # Append the results from each station to the respective DataFrame.\n",
    "    EvalDF_all_rf = pd.concat([EvalDF_all_rf, EvalDF_all_rf_temp], ignore_index=True)\n",
    "    SupplyEvalDF_all_rf = pd.concat([SupplyEvalDF_all_rf, SupplyEvalDF_all_rf_temp], ignore_index=True)\n",
    "    df_eval_rf_ = pd.concat([df_eval_rf, df_eval_rf_temp], ignore_index=True)\n",
    "\n",
    "print(\"Model Performance for Daily cfs\")\n",
    "display(EvalDF_all_rf)   \n",
    "print(\"Model Performance for Daily Accumulated Supply (Acre-Feet)\")\n",
    "display(SupplyEvalDF_all_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "general_viz(df_result_data, station_list, 0)\n",
    "general_viz(df_result_data, station_list, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2. Evaluation of different flow regimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regime_eval(df_result_data, station_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3. Hydrological Signatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signature_eval(df_result_data, station_list, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.4. Hydrological Drought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_result_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m----> 2\u001b[0m eval_drought(\u001b[43mdf_result_data\u001b[49m, station_list, duration, dataset, EvalDF_all_rf)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_result_data' is not defined"
     ]
    }
   ],
   "source": [
    "duration = 2\n",
    "eval_drought(df_result_data, station_list, duration, dataset, EvalDF_all_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d941b521942abff02888ea7873cca51c2aac5fb2f3b440dbf15a61d263ddb0d"
  },
  "kernelspec": {
   "display_name": "devcon_2025_cpu",
   "language": "python",
   "name": "devcon_2025_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
