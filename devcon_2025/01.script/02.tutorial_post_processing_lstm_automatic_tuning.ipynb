{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/envs/devcon_2025/lib/python3.12/site-packages/pyproj/__init__.py:89: UserWarning: pyproj unable to set database path.\n",
      "  _pyproj_global_context_initialize()\n"
     ]
    }
   ],
   "source": [
    "# my packages\n",
    "from evaluation_table import EvalTable\n",
    "from figure_generator import EvalPlot\n",
    "from model import CustomBiLSTM\n",
    "from tuning_tools import tuning_game, tune_model \n",
    "from data_preprocess import data_prepare, data_split\n",
    "from final_eval import general_viz, regime_eval, signature_eval, eval_drought\n",
    "\n",
    "# basic packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import math\n",
    "import joblib\n",
    "\n",
    "# system packages\n",
    "from datetime import datetime, date, timedelta\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import platform\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# hydrological packages\n",
    "import hydroeval as he\n",
    "from hydrotools.nwm_client import utils # I had to pip install this\n",
    "\n",
    "# data analysis packages\n",
    "from scipy import optimize\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "\n",
    "# deep learning packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Identify the path\n",
    "home = os.getcwd()\n",
    "parent_path = os.path.dirname(home)\n",
    "input_path = f'{parent_path}/02.input/'\n",
    "output_path = f'{parent_path}/03.output/'\n",
    "main_path = home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test dataset\n",
    "data_train = pd.read_pickle(f\"{output_path}train_dataset.pkl\")\n",
    "data_test = pd.read_pickle(f\"{output_path}test_dataset.pkl\")\n",
    "\n",
    "station_list = list(data_test.station_id.unique())\n",
    "\n",
    "length_lookback = 2\n",
    "x_train_scaled, y_train_scaled, x_test_scaled, y_test_scaled, scaler_x, scaler_y, y_train, x_test, y_test = data_prepare(data_train, data_test, length_lookback=length_lookback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Development \n",
    "#### 4.1. Defining the Model\n",
    "- As mentioned, we will use a Bidirectional LSTM model which has a simple two layer architecture and Pytorch library. \n",
    "- The first layer in our model is a bidirecional LSTM layer which is similar to normal LSTM and the only difference is that you have to turn 'bidirectional' variable to 'True' in the layer variables. \n",
    "- The second layer is fully connected layer which will get the ouptuts of LSTM layer, so we should multiple the neurans number (hidden_size variable) by two. \n",
    "\n",
    "* **`batch_size`** Batch size determines how many samples are processed before the model’s weights are updated. Smaller batches offer more frequent updates, while larger batches can provide more stable gradient estimates.\n",
    "\n",
    "* **`learning_rate`** The learning rate in neural networks controls how much the model’s weights are updated during training. A small learning rate leads to slower but more stable convergence, while a large one can speed up training but may cause the model to overshoot optimal solutions or diverge. It’s a critical hyperparameter that significantly affects training performance and outcomes.\n",
    "\n",
    "* **`hidden_size`** Hidden size refers to the number of units (neurons) in each hidden layer of the network, controlling the model’s capacity to learn patterns. In LSTMs, it defines the dimensionality of the hidden state and cell state.\n",
    "\n",
    "* **`num_layers`** Number of layers indicates how many stacked layers of LSTM cells the model has. More layers allow the network to learn more complex representations, but also increase the risk of overfitting and training instability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Tuning the Hyperparameters\n",
    "- We have several hyperparameters for our LSTM model, which we have to tune so that we can have the best possible results. \n",
    "- The tunning process can be done manually or by using optimization algorithms, in this tutorial we will use the values that we have identified to work best. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial score: 0.2021058531681804 with params: {'batch_size': 50, 'learning_rate': 0.0001, 'hidden_size': 300, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to change any variable? (y/n):  y\n",
      "Which variable number? (batch_size(1)/learning_rate(2)/hidden_size(3)/num_layers(4)): 4\n",
      "Enter the new value for num_layers (previous value 1):  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Mean Score: 0.202\n",
      "New Mean Score: 0.183 \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to keep the new variable?(y/n):  y\n",
      "Do you want to change any variable? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tuning.\n",
      "Final parameters: {'batch_size': 50, 'learning_rate': 0.0001, 'hidden_size': 300, 'num_layers': 2}.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_size': 50,\n",
       " 'learning_rate': 0.0001,\n",
       " 'hidden_size': 300,\n",
       " 'num_layers': 2}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "selected_station = station_list[0]\n",
    "epochs = 10 # We don't change it.\n",
    "input_size = x_train_scaled[selected_station].shape[2]\n",
    "# Move the model and data to GPU. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Feed the data to DataLoader and TensorDataset Functions\n",
    "x_train_tensor = torch.Tensor(x_train_scaled[selected_station].astype(float))\n",
    "y_train_tensor = torch.Tensor(y_train_scaled[selected_station].astype(float))\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "# Define the initial parameters for the LSTM model.\n",
    "params = {\n",
    "    'batch_size': 50,\n",
    "    'learning_rate': 1e-4,\n",
    "    'hidden_size': 300,\n",
    "    'num_layers': 1,\n",
    "}\n",
    "\n",
    "tuning_game(input_size, device, train_dataset, epochs, params, selected_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (6021) does not match length of index (6029)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m yhat_test \u001b[38;5;241m=\u001b[39m scaler_y\u001b[38;5;241m.\u001b[39minverse_transform(yhat_test_scaled\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Assuming EvalTable is a predefined function that compares predictions to actuals and returns evaluation DataFrames.\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m EvalDF_all_rf_temp, SupplyEvalDF_all_rf_temp, df_eval_rf_temp \u001b[38;5;241m=\u001b[39m \u001b[43mEvalTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43myhat_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstation_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mselected_station\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlstm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m df_result_data[selected_station] \u001b[38;5;241m=\u001b[39m data_test[data_test\u001b[38;5;241m.\u001b[39mstation_id \u001b[38;5;241m==\u001b[39m selected_station][\u001b[38;5;241m2\u001b[39m:]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     25\u001b[0m df_result_data[selected_station][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlstm_flow\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m yhat_test\n",
      "File \u001b[0;32m~/mydrive/devcon_2025/hydromachine-tutorials/neural_nets/lstm/01.script/evaluation_table.py:74\u001b[0m, in \u001b[0;36mEvalTable\u001b[0;34m(yhat_test, data_test, model_name)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mEvalTable\u001b[39m(yhat_test, data_test, model_name):\n\u001b[1;32m     73\u001b[0m     df_eval \u001b[38;5;241m=\u001b[39m data_test\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m---> 74\u001b[0m     \u001b[43mdf_eval\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_flow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m yhat_test\n\u001b[1;32m     75\u001b[0m     prediction_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNWM_flow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_flow\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     76\u001b[0m     observation_column \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflow_cms\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/envs/devcon_2025/lib/python3.12/site-packages/pandas/core/frame.py:4311\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4308\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4311\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/devcon_2025/lib/python3.12/site-packages/pandas/core/frame.py:4524\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4514\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4515\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4516\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4517\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4522\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4523\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4524\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4527\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4528\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4529\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4530\u001b[0m     ):\n\u001b[1;32m   4531\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4532\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/envs/devcon_2025/lib/python3.12/site-packages/pandas/core/frame.py:5266\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5266\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5267\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5268\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5269\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5270\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5273\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5274\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/devcon_2025/lib/python3.12/site-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (6021) does not match length of index (6029)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize empty DataFrames to store evaluation results if not already defined.\n",
    "EvalDF_all_rf = pd.DataFrame()\n",
    "SupplyEvalDF_all_rf = pd.DataFrame()\n",
    "df_eval_rf = pd.DataFrame()\n",
    "df_result_data= {}\n",
    "\n",
    "\n",
    "\n",
    "bilstm_model = CustomBiLSTM(input_size, params['hidden_size'], params['num_layers'], 1, device, embedding=False, station_list=station_list)\n",
    "\n",
    "x_test_tensor = torch.Tensor(x_test_scaled[selected_station].astype(float))\n",
    "y_test_tensor = torch.Tensor(y_test_scaled[selected_station].astype(float))\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_dataset.tensors[0].shape[0], shuffle=False)\n",
    "yhat_test_scaled, val_loss = bilstm_model.evaluate_model(test_loader)\n",
    "\n",
    "# Inverse transform the scaled predictions to their original scale.\n",
    "yhat_test = scaler_y.inverse_transform(yhat_test_scaled.reshape(-1, 1))\n",
    "\n",
    "# Assuming EvalTable is a predefined function that compares predictions to actuals and returns evaluation DataFrames.\n",
    "EvalDF_all_rf_temp, SupplyEvalDF_all_rf_temp, df_eval_rf_temp = EvalTable(yhat_test.reshape(-1), data_test[data_test.station_id == selected_station][2:], 'lstm')\n",
    "\n",
    "df_result_data[selected_station] = data_test[data_test.station_id == selected_station][2:].copy()\n",
    "\n",
    "df_result_data[selected_station]['lstm_flow'] = yhat_test\n",
    "\n",
    "# Append the results from each station to the respective DataFrame.\n",
    "EvalDF_all_rf = pd.concat([EvalDF_all_rf, EvalDF_all_rf_temp], ignore_index=True)\n",
    "SupplyEvalDF_all_rf = pd.concat([SupplyEvalDF_all_rf, SupplyEvalDF_all_rf_temp], ignore_index=True)\n",
    "df_eval_rf = pd.concat([df_eval_rf, df_eval_rf_temp], ignore_index=True)\n",
    "\n",
    "print(\"Model Performance for Daily cfs\")\n",
    "display(EvalDF_all_rf)   \n",
    "print(\"Model Performance for Daily Accumulated Supply (Acre-Feet)\")\n",
    "display(SupplyEvalDF_all_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Automatic Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-21 15:47:36,507] A new study created in memory with name: no-name-7301b78f-7cb6-46dc-b9f4-39c76a05b568\n",
      "[I 2025-05-21 15:47:38,812] Trial 0 finished with value: 0.13322460696967006 and parameters: {'hidden_size': 128, 'num_layers': 1, 'learning_rate': 0.003394573297864832}. Best is trial 0 with value: 0.13322460696967006.\n",
      "[I 2025-05-21 15:47:41,630] Trial 1 finished with value: 0.12717216416327595 and parameters: {'hidden_size': 42, 'num_layers': 2, 'learning_rate': 0.00843941366703184}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:47:46,311] Trial 2 finished with value: 0.14027943476563687 and parameters: {'hidden_size': 126, 'num_layers': 2, 'learning_rate': 0.0047380284580664424}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:47:52,323] Trial 3 finished with value: 0.13756137659486523 and parameters: {'hidden_size': 103, 'num_layers': 3, 'learning_rate': 0.0035406498999942366}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:47:54,541] Trial 4 finished with value: 0.15019762351972246 and parameters: {'hidden_size': 118, 'num_layers': 1, 'learning_rate': 0.00812478512049045}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:48:00,965] Trial 5 finished with value: 0.14563050486382845 and parameters: {'hidden_size': 111, 'num_layers': 3, 'learning_rate': 0.009930304284576029}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:48:03,664] Trial 6 finished with value: 0.13753829378478996 and parameters: {'hidden_size': 35, 'num_layers': 2, 'learning_rate': 0.005241241712515634}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:48:08,394] Trial 7 finished with value: 0.13464067224697965 and parameters: {'hidden_size': 66, 'num_layers': 3, 'learning_rate': 0.0012425579701172312}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:48:14,164] Trial 8 finished with value: 0.13597523005608997 and parameters: {'hidden_size': 95, 'num_layers': 3, 'learning_rate': 0.0035536200166522915}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:48:18,175] Trial 9 finished with value: 0.13910125533192894 and parameters: {'hidden_size': 43, 'num_layers': 3, 'learning_rate': 0.0017596897837012426}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:48:21,359] Trial 10 finished with value: 0.1368167383651364 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.007492483145668988}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:48:23,380] Trial 11 finished with value: 0.14818414254351372 and parameters: {'hidden_size': 83, 'num_layers': 1, 'learning_rate': 0.006689915945621573}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:48:25,242] Trial 12 finished with value: 0.13948067609566311 and parameters: {'hidden_size': 59, 'num_layers': 1, 'learning_rate': 0.00968507407697178}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:48:28,939] Trial 13 finished with value: 0.1289250293668743 and parameters: {'hidden_size': 84, 'num_layers': 2, 'learning_rate': 0.002826006738922746}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:48:32,585] Trial 14 finished with value: 0.14476788971311091 and parameters: {'hidden_size': 82, 'num_layers': 2, 'learning_rate': 0.0002751068444823506}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:48:35,662] Trial 15 finished with value: 0.12831286134371092 and parameters: {'hidden_size': 52, 'num_layers': 2, 'learning_rate': 0.005892032556331399}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:48:38,713] Trial 16 finished with value: 0.12912673674045352 and parameters: {'hidden_size': 53, 'num_layers': 2, 'learning_rate': 0.006107843779653023}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:48:41,682] Trial 17 finished with value: 0.1350676267883464 and parameters: {'hidden_size': 46, 'num_layers': 2, 'learning_rate': 0.008508087059238884}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:48:43,244] Trial 18 finished with value: 0.1311215703792407 and parameters: {'hidden_size': 32, 'num_layers': 1, 'learning_rate': 0.006706409329999726}. Best is trial 1 with value: 0.12717216416327595.\n",
      "[I 2025-05-21 15:48:46,599] Trial 19 finished with value: 0.14183252513651698 and parameters: {'hidden_size': 71, 'num_layers': 2, 'learning_rate': 0.008707967340885855}. Best is trial 1 with value: 0.12717216416327595.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_size': 42, 'num_layers': 2, 'learning_rate': 0.00843941366703184}\n"
     ]
    }
   ],
   "source": [
    "length_lookback = 10\n",
    "x_train_scaled, y_train_scaled, x_test_scaled, y_test_scaled, scaler_x, scaler_y, y_train, x_test, y_test = data_prepare(data_train, data_test, length_lookback=length_lookback)\n",
    "selected_station = station_list[0]\n",
    "epochs = 5 \n",
    "input_size = x_train_scaled[selected_station].shape[2]\n",
    "\n",
    "# Move the model and data to the GPU if available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Compute lengths for 80/20 split\n",
    "x_train_tensor = torch.Tensor(x_train_scaled[selected_station].astype(float))\n",
    "y_train_tensor = torch.Tensor(y_train_scaled[selected_station].astype(float))\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.8)\n",
    "val_len = len(train_dataset) - train_len\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_len, val_len])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = {selected_station: DataLoader(train_dataset, batch_size=50, shuffle=True)}\n",
    "val_loader = {selected_station: DataLoader(val_dataset, batch_size=50, shuffle=False)}\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=False)\n",
    "\n",
    "    # Create the Model\n",
    "    bilstm_model = CustomBiLSTM(input_size, hidden_size, num_layers, 1, device, embedding=False, station_list=station_list[0:1])\n",
    "    \n",
    "    # Create the Optimizer\n",
    "    bilstm_optimizer = optim.Adam(bilstm_model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "    \n",
    "    # Run the training function\n",
    "    model_parameters = bilstm_model.train_model(train_loader, epochs, bilstm_optimizer, early_stopping_patience=0, val_loader=None, tune='True')\n",
    "        # print('hi')\n",
    "    outputs, val_loss = bilstm_model.evaluate_model(val_loader[selected_station])\n",
    "    return val_loss  # Minimize validation loss\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/mydrive/devcon_2025/hydromachine-tutorials/neural_nets/lstm/03.output/best_hyperparameters_lstm.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = study.best_params\n",
    "params['batch_size'] = 50\n",
    "\n",
    "joblib.dump(params, f'{output_path}best_hyperparameters_lstm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**LETS GO TO THE NEXT PART**](./03.tutorial_post_processing_lstm_evaluation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d941b521942abff02888ea7873cca51c2aac5fb2f3b440dbf15a61d263ddb0d"
  },
  "kernelspec": {
   "display_name": "devcon_2025_cpu",
   "language": "python",
   "name": "devcon_2025_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
