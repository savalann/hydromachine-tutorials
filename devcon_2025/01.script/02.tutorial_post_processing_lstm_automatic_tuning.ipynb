{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my packages\n",
    "from evaluation_table import EvalTable\n",
    "from figure_generator import EvalPlot\n",
    "from model import CustomBiLSTM\n",
    "from tuning_tools import tuning_game, tune_model \n",
    "from data_preprocess import data_prepare, data_split\n",
    "from final_eval import general_viz, regime_eval, signature_eval, eval_drought\n",
    "\n",
    "# basic packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import math\n",
    "import joblib\n",
    "\n",
    "# system packages\n",
    "from datetime import datetime, date, timedelta\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import platform\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# hydrological packages\n",
    "import hydroeval as he\n",
    "from hydrotools.nwm_client import utils # I had to pip install this\n",
    "\n",
    "# data analysis packages\n",
    "from scipy import optimize\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "\n",
    "# deep learning packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Identify the path\n",
    "home = os.getcwd()\n",
    "parent_path = os.path.dirname(home)\n",
    "input_path = f'{parent_path}/02.input/'\n",
    "output_path = f'{parent_path}/03.output/'\n",
    "main_path = home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test dataset\n",
    "data_train = pd.read_pickle(f\"{output_path}train_dataset.pkl\")\n",
    "data_test = pd.read_pickle(f\"{output_path}test_dataset.pkl\")\n",
    "\n",
    "station_list = list(data_test.station_id.unique())\n",
    "\n",
    "length_lookback = 2\n",
    "x_train_scaled, y_train_scaled, x_test_scaled, y_test_scaled, scaler_x, scaler_y, y_train, x_test, y_test = data_prepare(data_train, data_test, length_lookback=length_lookback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Development \n",
    "#### 4.1. Defining the Model\n",
    "- As mentioned, we will use a Bidirectional LSTM model which has a simple two layer architecture and Pytorch library. \n",
    "- The first layer in our model is a bidirecional LSTM layer which is similar to normal LSTM and the only difference is that you have to turn 'bidirectional' variable to 'True' in the layer variables. \n",
    "- The second layer is fully connected layer which will get the ouptuts of LSTM layer, so we should multiple the neurans number (hidden_size variable) by two. \n",
    "\n",
    "* **`batch_size`** Batch size determines how many samples are processed before the model’s weights are updated. Smaller batches offer more frequent updates, while larger batches can provide more stable gradient estimates.\n",
    "\n",
    "* **`learning_rate`** The learning rate in neural networks controls how much the model’s weights are updated during training. A small learning rate leads to slower but more stable convergence, while a large one can speed up training but may cause the model to overshoot optimal solutions or diverge. It’s a critical hyperparameter that significantly affects training performance and outcomes.\n",
    "\n",
    "* **`hidden_size`** Hidden size refers to the number of units (neurons) in each hidden layer of the network, controlling the model’s capacity to learn patterns. In LSTMs, it defines the dimensionality of the hidden state and cell state.\n",
    "\n",
    "* **`num_layers`** Number of layers indicates how many stacked layers of LSTM cells the model has. More layers allow the network to learn more complex representations, but also increase the risk of overfitting and training instability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Tuning the Hyperparameters\n",
    "- We have several hyperparameters for our LSTM model, which we have to tune so that we can have the best possible results. \n",
    "- The tunning process can be done manually or by using optimization algorithms, in this tutorial we will use the values that we have identified to work best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range of different variables:\n",
    "- hidden_size  = (32, 128)\n",
    "- num_layers = (1, 3)\n",
    "- learning_rate = (1e-4, 1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Range for different variables:\n",
    "# hidden_size  = (32, 128)\n",
    "# num_layers = (1, 3)\n",
    "# learning_rate = (1e-4, 1e-2,)\n",
    "\n",
    "\n",
    "selected_station = station_list[0]\n",
    "epochs = 10 # We don't change it.\n",
    "input_size = x_train_scaled[selected_station].shape[2]\n",
    "# Move the model and data to GPU. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Feed the data to DataLoader and TensorDataset Functions\n",
    "x_train_tensor = torch.Tensor(x_train_scaled[selected_station].astype(float))\n",
    "y_train_tensor = torch.Tensor(y_train_scaled[selected_station].astype(float))\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "# Define the initial parameters for the LSTM model.\n",
    "params = {\n",
    "    'batch_size': 50,\n",
    "    'learning_rate': 1e-4,\n",
    "    'hidden_size': 300,\n",
    "    'num_layers': 1,\n",
    "}\n",
    "\n",
    "tuning_game(input_size, device, train_dataset, epochs, params, selected_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize empty DataFrames to store evaluation results if not already defined.\n",
    "EvalDF_all_rf = pd.DataFrame()\n",
    "SupplyEvalDF_all_rf = pd.DataFrame()\n",
    "df_eval_rf = pd.DataFrame()\n",
    "df_result_data= {}\n",
    "\n",
    "\n",
    "\n",
    "bilstm_model = CustomBiLSTM(input_size, params['hidden_size'], params['num_layers'], 1, device, embedding=False, station_list=station_list)\n",
    "\n",
    "x_test_tensor = torch.Tensor(x_test_scaled[selected_station].astype(float))\n",
    "y_test_tensor = torch.Tensor(y_test_scaled[selected_station].astype(float))\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_dataset.tensors[0].shape[0], shuffle=False)\n",
    "yhat_test_scaled, val_loss = bilstm_model.evaluate_model(test_loader)\n",
    "\n",
    "# Inverse transform the scaled predictions to their original scale.\n",
    "yhat_test = scaler_y.inverse_transform(yhat_test_scaled.reshape(-1, 1))\n",
    "\n",
    "# Assuming EvalTable is a predefined function that compares predictions to actuals and returns evaluation DataFrames.\n",
    "EvalDF_all_rf_temp, SupplyEvalDF_all_rf_temp, df_eval_rf_temp = EvalTable(yhat_test.reshape(-1), data_test[data_test.station_id == selected_station][2:], 'lstm')\n",
    "\n",
    "df_result_data[selected_station] = data_test[data_test.station_id == selected_station][2:].copy()\n",
    "\n",
    "df_result_data[selected_station]['lstm_flow'] = yhat_test\n",
    "\n",
    "# Append the results from each station to the respective DataFrame.\n",
    "EvalDF_all_rf = pd.concat([EvalDF_all_rf, EvalDF_all_rf_temp], ignore_index=True)\n",
    "SupplyEvalDF_all_rf = pd.concat([SupplyEvalDF_all_rf, SupplyEvalDF_all_rf_temp], ignore_index=True)\n",
    "df_eval_rf = pd.concat([df_eval_rf, df_eval_rf_temp], ignore_index=True)\n",
    "\n",
    "print(\"Model Performance for Daily cfs\")\n",
    "display(EvalDF_all_rf)   \n",
    "print(\"Model Performance for Daily Accumulated Supply (Acre-Feet)\")\n",
    "display(SupplyEvalDF_all_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Automatic Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "length_lookback = 10\n",
    "x_train_scaled, y_train_scaled, x_test_scaled, y_test_scaled, scaler_x, scaler_y, y_train, x_test, y_test = data_prepare(data_train, data_test, length_lookback=length_lookback)\n",
    "selected_station = station_list[0]\n",
    "epochs = 5 \n",
    "input_size = x_train_scaled[selected_station].shape[2]\n",
    "\n",
    "# Move the model and data to the GPU if available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Compute lengths for 80/20 split\n",
    "x_train_tensor = torch.Tensor(x_train_scaled[selected_station].astype(float))\n",
    "y_train_tensor = torch.Tensor(y_train_scaled[selected_station].astype(float))\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.8)\n",
    "val_len = len(train_dataset) - train_len\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_len, val_len])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = {selected_station: DataLoader(train_dataset, batch_size=50, shuffle=True)}\n",
    "val_loader = {selected_station: DataLoader(val_dataset, batch_size=50, shuffle=False)}\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=False)\n",
    "\n",
    "    # Create the Model\n",
    "    bilstm_model = CustomBiLSTM(input_size, hidden_size, num_layers, 1, device, embedding=False, station_list=station_list[0:1])\n",
    "    \n",
    "    # Create the Optimizer\n",
    "    bilstm_optimizer = optim.Adam(bilstm_model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "    \n",
    "    # Run the training function\n",
    "    model_parameters = bilstm_model.train_model(train_loader, epochs, bilstm_optimizer, early_stopping_patience=0, val_loader=None, tune='True')\n",
    "        # print('hi')\n",
    "    outputs, val_loss = bilstm_model.evaluate_model(val_loader[selected_station])\n",
    "    return val_loss  # Minimize validation loss\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = study.best_params\n",
    "params['batch_size'] = 50\n",
    "\n",
    "joblib.dump(params, f'{output_path}best_hyperparameters_lstm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**LETS GO TO THE NEXT PART**](./03.tutorial_post_processing_lstm_evaluation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d941b521942abff02888ea7873cca51c2aac5fb2f3b440dbf15a61d263ddb0d"
  },
  "kernelspec": {
   "display_name": "devcon_2025_cpu",
   "language": "python",
   "name": "devcon_2025_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
