{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my packages\n",
    "from evaluation_table import EvalTable\n",
    "from figure_generator import EvalPlot\n",
    "from model import CustomBiLSTM\n",
    "from tuning_tools import tuning_game, tune_model \n",
    "from data_preprocess import data_prepare, data_split\n",
    "from final_eval import general_viz, regime_eval, signature_eval, eval_drought\n",
    "\n",
    "# basic packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import math\n",
    "import joblib\n",
    "\n",
    "# system packages\n",
    "from datetime import datetime, date, timedelta\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import platform\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# hydrological packages\n",
    "import hydroeval as he\n",
    "from hydrotools.nwm_client import utils # I had to pip install this\n",
    "\n",
    "# data analysis packages\n",
    "from scipy import optimize\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "\n",
    "# deep learning packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Identify the path\n",
    "home = os.getcwd()\n",
    "parent_path = os.path.dirname(home)\n",
    "input_path = f'{parent_path}/02.input/'\n",
    "output_path = f'{parent_path}/03.output/'\n",
    "main_path = home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test dataset\n",
    "data_train = pd.read_pickle(f\"{output_path}train_dataset.pkl\")\n",
    "data_test = pd.read_pickle(f\"{output_path}test_dataset.pkl\")\n",
    "\n",
    "station_list = list(data_test.station_id.unique())\n",
    "\n",
    "length_lookback = 2\n",
    "x_train_scaled, y_train_scaled, x_test_scaled, y_test_scaled, scaler_x, scaler_y, y_train, x_test, y_test = data_prepare(data_train, data_test, length_lookback=length_lookback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Development \n",
    "#### 4.1. Defining the Model\n",
    "- As mentioned, we will use a Bidirectional LSTM model which has a simple two layer architecture and Pytorch library. \n",
    "- The first layer in our model is a bidirecional LSTM layer which is similar to normal LSTM and the only difference is that you have to turn 'bidirectional' variable to 'True' in the layer variables. \n",
    "- The second layer is fully connected layer which will get the ouptuts of LSTM layer, so we should multiple the neurans number (hidden_size variable) by two. \n",
    "\n",
    "* **`batch_size`** Batch size determines how many samples are processed before the model’s weights are updated. Smaller batches offer more frequent updates, while larger batches can provide more stable gradient estimates.\n",
    "\n",
    "* **`learning_rate`** The learning rate in neural networks controls how much the model’s weights are updated during training. A small learning rate leads to slower but more stable convergence, while a large one can speed up training but may cause the model to overshoot optimal solutions or diverge. It’s a critical hyperparameter that significantly affects training performance and outcomes.\n",
    "\n",
    "* **`hidden_size`** Hidden size refers to the number of units (neurons) in each hidden layer of the network, controlling the model’s capacity to learn patterns. In LSTMs, it defines the dimensionality of the hidden state and cell state.\n",
    "\n",
    "* **`num_layers`** Number of layers indicates how many stacked layers of LSTM cells the model has. More layers allow the network to learn more complex representations, but also increase the risk of overfitting and training instability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. Tuning the Hyperparameters\n",
    "- We have several hyperparameters for our LSTM model, which we have to tune so that we can have the best possible results. \n",
    "- The tunning process can be done manually or by using optimization algorithms, in this tutorial we will use the values that we have identified to work best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Range of different variables:\n",
    "- hidden_size  = (32, 128)\n",
    "- num_layers = (1, 3)\n",
    "- learning_rate = (1e-4, 1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial score: 0.07227395497253071 with params: {'batch_size': 50, 'learning_rate': 0.0001, 'hidden_size': 300, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you want to change any variable? (y/n):  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tuning.\n",
      "Final parameters: {'batch_size': 50, 'learning_rate': 0.0001, 'hidden_size': 300, 'num_layers': 1}.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Range for different variables:\n",
    "# hidden_size  = (32, 128)\n",
    "# num_layers = (1, 3)\n",
    "# learning_rate = (1e-4, 1e-2,)\n",
    "\n",
    "\n",
    "selected_station = station_list[0]\n",
    "epochs = 10 # We don't change it.\n",
    "input_size = x_train_scaled[selected_station].shape[2]\n",
    "# Move the model and data to GPU. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Feed the data to DataLoader and TensorDataset Functions\n",
    "x_train_tensor = torch.Tensor(x_train_scaled[selected_station].astype(float))\n",
    "y_train_tensor = torch.Tensor(y_train_scaled[selected_station].astype(float))\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "# Define the initial parameters for the LSTM model.\n",
    "params = {\n",
    "    'batch_size': 50,\n",
    "    'learning_rate': 1e-4,\n",
    "    'hidden_size': 300,\n",
    "    'num_layers': 1,\n",
    "}\n",
    "\n",
    "tuning_game(input_size, device, train_dataset, epochs, params, selected_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance for Daily cfs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USGSid</th>\n",
       "      <th>NHDPlusid</th>\n",
       "      <th>NWM_RMSE</th>\n",
       "      <th>lstm_RMSE</th>\n",
       "      <th>NWM_PBias</th>\n",
       "      <th>lstm_PBias</th>\n",
       "      <th>NWM_KGE</th>\n",
       "      <th>lstm_KGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10131000</td>\n",
       "      <td>10375648</td>\n",
       "      <td>7.24</td>\n",
       "      <td>7.48</td>\n",
       "      <td>-359.07</td>\n",
       "      <td>-375.1</td>\n",
       "      <td>-2.62</td>\n",
       "      <td>-3.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     USGSid NHDPlusid NWM_RMSE lstm_RMSE NWM_PBias lstm_PBias NWM_KGE lstm_KGE\n",
       "0  10131000  10375648     7.24      7.48   -359.07     -375.1   -2.62     -3.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance for Daily Accumulated Supply (Acre-Feet)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>USGSid</th>\n",
       "      <th>NHDPlusid</th>\n",
       "      <th>NWM_RMSE</th>\n",
       "      <th>lstm_RMSE</th>\n",
       "      <th>NWM_PBias</th>\n",
       "      <th>lstm_PBias</th>\n",
       "      <th>NWM_KGE</th>\n",
       "      <th>lstm_KGE</th>\n",
       "      <th>Obs_vol</th>\n",
       "      <th>NWM_vol</th>\n",
       "      <th>lstm_vol</th>\n",
       "      <th>NWM_vol_err</th>\n",
       "      <th>lstm_vol_err</th>\n",
       "      <th>NWM_vol_Perc_diff</th>\n",
       "      <th>lstm_vol_Perc_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10131000</td>\n",
       "      <td>10375648</td>\n",
       "      <td>98020.14</td>\n",
       "      <td>94364.04</td>\n",
       "      <td>-327.34</td>\n",
       "      <td>-310.06</td>\n",
       "      <td>-2.78</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>45622.76</td>\n",
       "      <td>225028.66</td>\n",
       "      <td>213788.42</td>\n",
       "      <td>179405.9</td>\n",
       "      <td>168165.66</td>\n",
       "      <td>393.24</td>\n",
       "      <td>368.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     USGSid NHDPlusid  NWM_RMSE lstm_RMSE NWM_PBias lstm_PBias NWM_KGE  \\\n",
       "0  10131000  10375648  98020.14  94364.04   -327.34    -310.06   -2.78   \n",
       "\n",
       "  lstm_KGE   Obs_vol    NWM_vol   lstm_vol NWM_vol_err lstm_vol_err  \\\n",
       "0     -2.5  45622.76  225028.66  213788.42    179405.9    168165.66   \n",
       "\n",
       "  NWM_vol_Perc_diff lstm_vol_Perc_diff  \n",
       "0            393.24              368.6  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Initialize empty DataFrames to store evaluation results if not already defined.\n",
    "EvalDF_all_rf = pd.DataFrame()\n",
    "SupplyEvalDF_all_rf = pd.DataFrame()\n",
    "df_eval_rf = pd.DataFrame()\n",
    "df_result_data= {}\n",
    "\n",
    "\n",
    "\n",
    "bilstm_model = CustomBiLSTM(input_size, params['hidden_size'], params['num_layers'], 1, device, embedding=False, station_list=station_list)\n",
    "\n",
    "x_test_tensor = torch.Tensor(x_test_scaled[selected_station].astype(float))\n",
    "y_test_tensor = torch.Tensor(y_test_scaled[selected_station].astype(float))\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_dataset.tensors[0].shape[0], shuffle=False)\n",
    "yhat_test_scaled, val_loss = bilstm_model.evaluate_model(test_loader)\n",
    "\n",
    "# Inverse transform the scaled predictions to their original scale.\n",
    "yhat_test = scaler_y.inverse_transform(yhat_test_scaled.reshape(-1, 1))\n",
    "\n",
    "# Assuming EvalTable is a predefined function that compares predictions to actuals and returns evaluation DataFrames.\n",
    "EvalDF_all_rf_temp, SupplyEvalDF_all_rf_temp, df_eval_rf_temp = EvalTable(yhat_test.reshape(-1), data_test[data_test.station_id == selected_station][2:], 'lstm')\n",
    "\n",
    "df_result_data[selected_station] = data_test[data_test.station_id == selected_station][2:].copy()\n",
    "\n",
    "df_result_data[selected_station]['lstm_flow'] = yhat_test\n",
    "\n",
    "# Append the results from each station to the respective DataFrame.\n",
    "EvalDF_all_rf = pd.concat([EvalDF_all_rf, EvalDF_all_rf_temp], ignore_index=True)\n",
    "SupplyEvalDF_all_rf = pd.concat([SupplyEvalDF_all_rf, SupplyEvalDF_all_rf_temp], ignore_index=True)\n",
    "df_eval_rf = pd.concat([df_eval_rf, df_eval_rf_temp], ignore_index=True)\n",
    "\n",
    "print(\"Model Performance for Daily cfs\")\n",
    "display(EvalDF_all_rf)   \n",
    "print(\"Model Performance for Daily Accumulated Supply (Acre-Feet)\")\n",
    "display(SupplyEvalDF_all_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Automatic Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 04:34:08,996] A new study created in memory with name: no-name-ec9236f6-3fdd-426a-8e8d-6556e6b1524f\n",
      "[I 2025-05-29 04:34:16,229] Trial 0 finished with value: 0.05176937607982402 and parameters: {'hidden_size': 111, 'num_layers': 3, 'learning_rate': 0.005969931839958808}. Best is trial 0 with value: 0.05176937607982402.\n",
      "[I 2025-05-29 04:34:21,530] Trial 1 finished with value: 0.04181307280027469 and parameters: {'hidden_size': 50, 'num_layers': 3, 'learning_rate': 0.008575497601372424}. Best is trial 1 with value: 0.04181307280027469.\n",
      "[I 2025-05-29 04:34:25,842] Trial 2 finished with value: 0.0477247014679429 and parameters: {'hidden_size': 75, 'num_layers': 2, 'learning_rate': 0.0042138135281977445}. Best is trial 1 with value: 0.04181307280027469.\n",
      "[I 2025-05-29 04:34:29,632] Trial 3 finished with value: 0.043089929320242096 and parameters: {'hidden_size': 42, 'num_layers': 2, 'learning_rate': 0.00838736021887823}. Best is trial 1 with value: 0.04181307280027469.\n",
      "[I 2025-05-29 04:34:32,328] Trial 4 finished with value: 0.04615861905175586 and parameters: {'hidden_size': 64, 'num_layers': 1, 'learning_rate': 0.006556209646477005}. Best is trial 1 with value: 0.04181307280027469.\n",
      "[I 2025-05-29 04:34:39,805] Trial 5 finished with value: 0.038678615426510965 and parameters: {'hidden_size': 122, 'num_layers': 3, 'learning_rate': 0.002696401045607767}. Best is trial 5 with value: 0.038678615426510965.\n",
      "[I 2025-05-29 04:34:44,023] Trial 6 finished with value: 0.0441272230206775 and parameters: {'hidden_size': 64, 'num_layers': 2, 'learning_rate': 0.004079502082636508}. Best is trial 5 with value: 0.038678615426510965.\n",
      "[I 2025-05-29 04:34:48,924] Trial 7 finished with value: 0.04236960518423419 and parameters: {'hidden_size': 79, 'num_layers': 2, 'learning_rate': 0.0013762206274092192}. Best is trial 5 with value: 0.038678615426510965.\n",
      "[I 2025-05-29 04:34:53,627] Trial 8 finished with value: 0.04567390652816023 and parameters: {'hidden_size': 77, 'num_layers': 2, 'learning_rate': 0.004937569962200945}. Best is trial 5 with value: 0.038678615426510965.\n",
      "[I 2025-05-29 04:34:57,204] Trial 9 finished with value: 0.04869597309897112 and parameters: {'hidden_size': 35, 'num_layers': 2, 'learning_rate': 0.0004299808692476288}. Best is trial 5 with value: 0.038678615426510965.\n",
      "[I 2025-05-29 04:35:04,546] Trial 10 finished with value: 0.0523375731688539 and parameters: {'hidden_size': 125, 'num_layers': 3, 'learning_rate': 0.002491929425775429}. Best is trial 5 with value: 0.038678615426510965.\n",
      "[I 2025-05-29 04:35:11,107] Trial 11 finished with value: 0.06180732966945916 and parameters: {'hidden_size': 102, 'num_layers': 3, 'learning_rate': 0.009645880874436902}. Best is trial 5 with value: 0.038678615426510965.\n",
      "[I 2025-05-29 04:35:17,422] Trial 12 finished with value: 0.04571729162058165 and parameters: {'hidden_size': 98, 'num_layers': 3, 'learning_rate': 0.007996066737871118}. Best is trial 5 with value: 0.038678615426510965.\n",
      "[I 2025-05-29 04:35:22,627] Trial 13 finished with value: 0.04169058866414672 and parameters: {'hidden_size': 50, 'num_layers': 3, 'learning_rate': 0.003415970803619818}. Best is trial 5 with value: 0.038678615426510965.\n",
      "[I 2025-05-29 04:35:25,925] Trial 14 finished with value: 0.04271989272807558 and parameters: {'hidden_size': 127, 'num_layers': 1, 'learning_rate': 0.002628132158867409}. Best is trial 5 with value: 0.038678615426510965.\n",
      "[I 2025-05-29 04:35:32,621] Trial 15 finished with value: 0.056368353943311286 and parameters: {'hidden_size': 95, 'num_layers': 3, 'learning_rate': 0.0028766926441774147}. Best is trial 5 with value: 0.038678615426510965.\n",
      "[I 2025-05-29 04:35:37,917] Trial 16 finished with value: 0.04008403184651364 and parameters: {'hidden_size': 54, 'num_layers': 3, 'learning_rate': 0.001464496147608632}. Best is trial 5 with value: 0.038678615426510965.\n",
      "[I 2025-05-29 04:35:45,044] Trial 17 finished with value: 0.04866743372156237 and parameters: {'hidden_size': 115, 'num_layers': 3, 'learning_rate': 0.001174608929587487}. Best is trial 5 with value: 0.038678615426510965.\n",
      "[I 2025-05-29 04:35:48,111] Trial 18 finished with value: 0.0518153921500436 and parameters: {'hidden_size': 89, 'num_layers': 1, 'learning_rate': 0.00023662051193919795}. Best is trial 5 with value: 0.038678615426510965.\n",
      "[I 2025-05-29 04:35:53,736] Trial 19 finished with value: 0.04825379530342693 and parameters: {'hidden_size': 63, 'num_layers': 3, 'learning_rate': 0.0017507430090762492}. Best is trial 5 with value: 0.038678615426510965.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_size': 122, 'num_layers': 3, 'learning_rate': 0.002696401045607767}\n"
     ]
    }
   ],
   "source": [
    "length_lookback = 10\n",
    "x_train_scaled, y_train_scaled, x_test_scaled, y_test_scaled, scaler_x, scaler_y, y_train, x_test, y_test = data_prepare(data_train, data_test, length_lookback=length_lookback)\n",
    "selected_station = station_list[0]\n",
    "epochs = 5 \n",
    "input_size = x_train_scaled[selected_station].shape[2]\n",
    "\n",
    "# Move the model and data to the GPU if available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Compute lengths for 80/20 split\n",
    "x_train_tensor = torch.Tensor(x_train_scaled[selected_station].astype(float))\n",
    "y_train_tensor = torch.Tensor(y_train_scaled[selected_station].astype(float))\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.8)\n",
    "val_len = len(train_dataset) - train_len\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_len, val_len])\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = {selected_station: DataLoader(train_dataset, batch_size=50, shuffle=True)}\n",
    "val_loader = {selected_station: DataLoader(val_dataset, batch_size=50, shuffle=False)}\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=False)\n",
    "\n",
    "    # Create the Model\n",
    "    bilstm_model = CustomBiLSTM(input_size, hidden_size, num_layers, 1, device, embedding=False, station_list=station_list[0:1])\n",
    "    \n",
    "    # Create the Optimizer\n",
    "    bilstm_optimizer = optim.Adam(bilstm_model.parameters(), lr=learning_rate, weight_decay=0)\n",
    "    \n",
    "    # Run the training function\n",
    "    model_parameters = bilstm_model.train_model(train_loader, epochs, bilstm_optimizer, early_stopping_patience=0, val_loader=None, tune='True')\n",
    "        # print('hi')\n",
    "    outputs, val_loss = bilstm_model.evaluate_model(val_loader[selected_station])\n",
    "    return val_loss  # Minimize validation loss\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jovyan/mydrive/devcon_2025/final/hydromachine-tutorials/devcon_2025/03.output/best_hyperparameters_lstm.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = study.best_params\n",
    "params['batch_size'] = 50\n",
    "\n",
    "joblib.dump(params, f'{output_path}best_hyperparameters_lstm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**LETS GO TO THE NEXT PART**](./03.tutorial_post_processing_lstm_evaluation.ipynb)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d941b521942abff02888ea7873cca51c2aac5fb2f3b440dbf15a61d263ddb0d"
  },
  "kernelspec": {
   "display_name": "devcon_2025_cpu",
   "language": "python",
   "name": "devcon_2025_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
